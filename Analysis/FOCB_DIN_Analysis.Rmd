---
title: "Analysis of Friends of Casco Bay DIN Data"
author: "Curtis C. Bohlen, Casco Bay Estuary Partnership."
date: "04/26/2021"
output:
  github_document:
    toc: true
    fig_width: 5
    fig_height: 4
---

<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:10px;right:50px;" />

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center',
                      fig.width = 5, fig.height = 4,
                      collapse = TRUE, comment = "#>")
```

# Introduction
This notebook Looks at DIN numbers from Friends of Casco Bay samples.

FOCB reports the TN samples and DIN samples were sent to different laboratories,
and so direct comparison relies on consistent calibration, etc. across two labs.
Accordingly, here we restrict our analysis to looking at DIN. A separate 
notebook looks at TN. 

FOCB reports that some DIN samples over the years had unusually high
ammonium values, and that those samples were noted by the laboratory conducting
the analyses, but not flagged as errors.  We created a data set that dropped
the top 5% of ammonium valuesd and ammonium data where DIN was larger than TN.
Details are in the "FOCB_Nutrients_Combined.Rmd" notebook.

# Load Libraries
```{r}
library(MASS) # for `rlm()` ans `lqs()`for robust regression
              # also `cov.rob()` for robust multivariate scatter and covariance.
              # Because MASS contains a function `select()` that conflicts with
              # the tidyverse `select()` function, `MASS` should be loaded before
              # the tidyverse.

library(readxl)
library(tidyverse)

library(mgcv)    # For generalized linear models

library(emmeans)
library(moments)  # for skewness and kurtosis)

library(sfsmisc)  # Provides alternative access to wald test for robust models

library(CBEPgraphics)
load_cbep_fonts()
theme_set(theme_cbep())
```

# Load Data
## Folder References
```{r folder_refs}
sibfldnm <- 'Derived_Data'
parent <- dirname(getwd())
sibling <- file.path(parent,sibfldnm)

#dir.create(file.path(getwd(), 'figures'), showWarnings = FALSE)
```

## Load Data
The data we use here has had a number of suspiciously high NH4 values removed.
See "FOCB_Nutrients_Combined.Rmd" for details and explanation.
```{r load_data}
strict_data <- read_csv(file.path(sibling, 
                                 "focb_n_data_strict.csv"))%>%
  mutate(month = factor(month, levels = month.abb),
         yearf = factor(year)) %>%
  mutate(dt = as.Date(dt))
```

# Station Names
```{r folder_refs_2}
fn <- 'FOCB Monitoring Sites SHORT NAMES.xlsx'
names_df <- read_excel(file.path(sibling, fn))
```

# Data Review
## Data Distributions
```{r din_hist}
ggplot(strict_data , aes(din)) +
  geom_histogram()
```

A log transform is "too stong" for the complete DIN data and leaves the data 
skewed the other way.
```{r din_site_hist}
ggplot(strict_data , aes(din)) +
  geom_histogram(aes(fill = station)) +
  theme(legend.position = 'none') +
  scale_x_log10()
```

We can partially correct by using a generalized log transform, although
selection of the additive constant is fairly arbitrary. A value between 0.75 and 
2  appears to work fairly well.

This assessment of the value of the log + k transform changes with some data
subsets, below.  many analyses have better model diagnostics on the log 
transform.
```{r din_log_hist}
glog = function(.x, .k) log(.x + .k)

ggplot(strict_data , aes(glog(din, 1.5))) +
  geom_histogram(aes(fill = station)) +
  theme(legend.position = 'none')
```

But the log plus one transform looks pretty good for most stations.  Things get 
complex with later models, on restricted data, where the log transform performs
slightly better.

```{r facet_din_densities, fig.width = 7, fig.height = 5}
ggplot(strict_data , aes(log1p(din))) +
  geom_density(aes(fill = station)) +
  facet_wrap(~ station) +
  theme_minimal() +         # restores gridlines
  theme(legend.position = 'none')
```
A number of sites show tendencies towards bimodal distributions of DIN. Later 
analyses suggest that may reflect seasonal patterns.

## Cross- Plot DIN by TN
```{r tn_din_plot_ammonium_strict, fig.height = 5, fig.width = 7}
ggplot(strict_data, aes(tn, din_N)) + 
  geom_point(aes(fill = month), size = 2, shape = 21, alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1) +
  #scale_fill_manual(values = cbep_colors()) +
  coord_equal() +
  theme_cbep(base_size = 12) +
    ylab('DIN (mg/ l as N)') +
    xlab('TN (mg/l)') +
  xlim(0, 1.5)
```
# Recent Conditions
Recent conditions include data from 2015 through 2019.

We remove the data for KVL84 from these analyses, because we have very 
limited local data from that site.

```{r create_recent_data}
recent_data <- strict_data %>%
  filter(year > 2014) %>%
  filter(station != 'KVL84')
```

## Add Shortened Site Names
The key step here is reordering by median nitrogen values. That ordering will
need to be harmonized with TN ordering to generate final graphics.
```{r add_site_names}
recent_data <- recent_data %>%
   mutate(station_name = names_df$Alt_Name[match(station,
                                                names_df$Station_ID)]) %>%
   mutate(station = factor(station),
          station_name = factor(station_name)) %>%
   mutate(station = fct_reorder(station, tn, na.rm = TRUE),
         station_name = fct_reorder(station_name, tn, na.rm = TRUE)) %>%
   relocate(station_name, .after = station) %>%
   select(-tn_depth, -tn, -organic_N)
```

## Data Prevalence
```{r crosstabs}
xtabs(~station + year, data = strict_data[! is.na(strict_data$din),])
```
DIN data has been collected fairly consistently from a handful of sites over 
many years, and from many sites only in 2019.  Samples have been collected at 
different times of year as well, complicating analyses, as year, station, and
season / time of year are confounded.

```{r recent_data_months}
xtabs(~ month + station, data = recent_data )%>%
  as_tibble() %>%
  mutate(month = factor(month, levels = month.abb)) %>%
  filter(n>0) %>%

  ggplot(aes(station, month, fill = sqrt(n))) +
  geom_tile() +
  theme_cbep(base_size = 12) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .25))
```

```{r recent_data_years}
xtabs(~ year + station, data = recent_data) %>%
  as_tibble() %>% 
  filter(n>0) %>%

  ggplot(aes(station, year, fill = sqrt(n))) +
  geom_tile() +
  theme_cbep(base_size = 12) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .25))
```

```{r recent_data_times}
xtabs(~ year + month, data = recent_data) %>%
  as_tibble() %>% 
  mutate(month = factor(month, levels = month.abb))  %>%
  filter(n>0) %>%

  ggplot(aes(month, year, fill = sqrt(n))) +
  geom_tile() +
  theme_cbep(base_size = 12) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .25))
```

We note that several stations have fewer than ten DIN samples over the
recent period, and sampling has focused on a smaller number of sites,
a smaller number of months, or both.

Only one site (KVL84, Knightville Landing, in South Portland) has fewer 
than five DIN values.  It was dropped, above, for lack of recent data.

With the relatively low sample sizes and uneven sampling histories for most
sites, complex models may perform poorly.  Interactions with year, time of year,
and location, in particular, will lead to many empty cells in the implicit 
model design.  Those may lead to a variety of model artifacts.

## Extract Recent Results
This is the simplest analysis, with no hierarchical modeling.  We drop the 
extreme TN values, ass we do for most analyses coming up.
```{r recent_results}
recent_results <- recent_data %>%
  group_by(station) %>%
  summarize(across(nox:nh4_N, c(mn = ~ mean(.x, na.rm = TRUE),
                                  sd = ~ sd(.x, na.rm = TRUE), 
                                  n = ~sum(! is.na(.x)),
                                  md = ~ median(.x, na.rm = TRUE),
                                  iqr = ~ IQR(.x, na.rm = TRUE),
                                  p90 = ~ quantile(.x, .9, na.rm = TRUE),
                                  gm = ~ exp(mean(log(.x), na.rm = TRUE))))) %>%
  mutate(station_name = names_df$Alt_Name[match(station,
                                                names_df$Station_ID)]) %>%
  mutate(station = fct_reorder(factor(station), din_md),
         station_name = fct_reorder(factor(station_name), din_md)) %>%
  relocate(station_name, .after = station)
```

# Models
We want to look at recent conditions, taking into account as best we can 
possible covariates, including year and time of year.  Our goal is to extract
means, medians, or marginal means by station for the recent data to plot on 
graphics and GIS.

# Recent Data
## Linear Model
We begin by constructing conventional linear models on log transformed DIN data.
```{r full_lm}
full_din_lm <- lm(log(din_N) ~ station *  month + yearf, data = recent_data)
anova(full_din_lm)
```

Stepwise model selection confirms that the interaction term is of little value
(by AIC; not shown) .
```{r reduced_lm}
din_lm <- lm(log(din_N) ~ station +  month + yearf, data = recent_data)
anova(din_lm)
```

### Model Diagnostics
```{r reduced_lm_diagnostics}
oldpar <- par(mfrow = c(2,2))
plot(din_lm)
par(oldpar)
```
We have a few values that are fairly badly underestimated (large negative 
residuals), but diagnostics are not dreadful. Residuals are somewhat heavy 
tailed.  There is some evidence that a plain log transform has over corrected
the skew in the original data.

```{r look_at_outliers}
recent_data[c(149, 430, 501),]
```
The poorly fit samples are all from 2015 and 2016 during the warm season.  It is
possible the fit for those years is affected by the prevalence of winter
samples, with the unbalanced sampling history biasing estimates for the entire
year.

### Marginal Means
```{r extract_lm_marginal_means}
din_emms_lm <- emmeans(din_lm, 'station', type = 'response')
din_emms_lm_jul <- emmeans(din_lm, 'station', type = 'response', 
                       at = list(month = 'Jul'))
```

Unfortunately, the relationship to observed means is only so-so.  Again, this
probably reflects unbalanced data, leading to large "corrections" in some
places where available data are biased.
```{r}
plot(din_emms_lm) + coord_flip() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) +
  geom_point(data = recent_results, aes(y = station, x = din_N_md),
             color = 'red')
```

### Compare to Observed Means
Unfortunately, this model does a fairly poor job regenerating the 
means from the original data, and standard errors for some estimates are 
very large (note the log-log plot).  This suggests this model is performing 
poorly.
```{r compare_lm_observed}
compare <- recent_results %>%
  select(station, station_name, contains('tn'), contains('din_N')) %>%
  full_join(din_emms_lm, by = 'station', suffix = c('.data', '.lm'), copy = TRUE)

ggplot(compare, aes(din_N_mn, response)) +
  geom_abline(slope = 1, intercept = 0) + 
  geom_point(size = 3, color = 'blue') +
  geom_linerange(aes(xmin = din_N_mn - 2 * din_N_sd/sqrt(din_N_n), 
                     xmax = din_N_mn + 2 * din_N_sd/sqrt(din_N_n))) +
  geom_linerange(aes(ymin = lower.CL, ymax = upper.CL)) +
  xlab('Observed ') +
  ylab('Log Linear Model') +
  coord_equal() +
  scale_x_log10() +
  scale_y_log10()
```

## Simplified Linear Models
We look at a linear models that do not include as many predictors, and show that 
they return marginal means that DO match the observed means fairly well.
```{r red_lm}
red_din_lm  <- lm(log(din_N) ~ station, data = recent_data)
din_emms_red_lm     <- emmeans(red_din_lm,     'station', type = 'response')
```

### Compare to Observed Means
```{r compare_red_lm_observed}
compare <- recent_results %>%
  select(station, station_name, contains('tn'), contains('din_N')) %>%
  full_join(din_emms_red_lm, by = 'station', suffix = c('.data', '.lm'), copy = TRUE)

ggplot(compare, aes(din_N_mn, response)) +
  geom_abline(slope = 1, intercept = 0) + 
  geom_point(size = 3, color = 'blue') +
  geom_linerange(aes(xmin = din_N_mn - 2 * din_N_sd/sqrt(din_N_n), 
                     xmax = din_N_mn + 2 * din_N_sd/sqrt(din_N_n))) +
  geom_linerange(aes(ymin = lower.CL, ymax = upper.CL)) +
  xlab('Observed ') +
  ylab('Linear Model') +
  coord_equal() +
  scale_x_log10() +
  scale_y_log10()
```
Standard errors are still fairly large, but the model at least provides
results closer to observed means.  Error bars tend to overlap the 1:1 line.
The differences here probably reflect the differences between geometric and 
arithmetic means.

All this suggests we are slicing these data too finely, and we should take a 
different approach. We have good data coverage from 2019, so we can restrict 
attention to 2019.

## GAM Model
We can use a GAM model to look more closely at seasonal patterns.
```{r gam_lodel}
din_gam <- gam(log(din_N) ~ station +  s(doy, bs = 'cc', k = 5) + 
                                         s(yearf, bs = 're'), 
               data = recent_data)
anova(din_gam)
```

```{r view_smoothers}
plot(din_gam)
```
The day of year smoother may be slightly over fit here, and we have not fully
explored interactions between seasonal effects and station.

```{r gam_diagnostics}
oldpar <- par(mfrow = c(2,2))
gam.check(din_gam)
par(oldpar)
```
Those diagnostics are not too bad, with the exception of a few large negative 
residuals again.

#### GAM Marginal Means
```{r gam_marginals}
din_emms_gam <- emmeans(din_gam, 'station', type = 'response', 
                        cov_reduce = median,
                        cov_keep = 'year',
                        at = list(doy = 200))
plot(din_emms_gam) + coord_flip() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25))
din_emms_gam <- as_tibble(din_emms_gam)
```

### Compare to Observed Mean
```{r compare_gam_observed}
compare <- recent_results %>%
  select(station, station_name, contains('tn'), contains('din_N')) %>%
  full_join(din_emms_gam, by = 'station', suffix = c('.data', '.lm'), copy = TRUE)

ggplot(compare, aes(din_N_mn, response)) +
  geom_abline(slope = 1, intercept = 0) + 
  geom_point(size = 3, color = 'blue') +
  geom_linerange(aes(xmin = din_N_mn - 2 * din_N_sd/sqrt(din_N_n), 
                     xmax = din_N_mn + 2 * din_N_sd/sqrt(din_N_n))) +
  geom_linerange(aes(ymin = lower.CL, ymax = upper.CL)) +
  xlab('Observed ') +
  ylab('Log Linear Model') +
  coord_equal() +
  scale_x_log10() +
  scale_y_log10()
```

This does better than the linear model, but only slightly.

## Sample Sizes are Wildly Unequal
Sample sizes are wildly unequal....
```{r review_uneven_samples_month}
xtabs(~ year + month, data = recent_data , subset = ! is.na(din))
```

So, seasonal trends outside of summer rest on just a handful of observations in
each of 2015 and 2016.

```{r  review_uneven_samples_sites}
xtabs(~ station + year, data = recent_data , subset = ! is.na(din))
```
And those observations come from just a handful of sites.

The problem here is the uneven sampling history.  We are trying to overinterpret
available data, leading to statistically unstable estimates.

# Restricted DIN Data (Year = 2019)
Our primary goal is to provide a map and accompanying chart of DIN levels.  For
that, we want to compare all sites on an even footing.  We now know that there
are important annual and seasonal processes at work, so the uneven sampling
history affects estimates of site conditions.

Data coverage in 2019 is fairly consistent.  Coverage is sparse, but
consistent across stations (but not months) in 2017 as well.

We restrict further attention to just 2019, as that data will not be affected by 
the uneven sampling history to the same extent.  Later we will look at just the 
warmer months of the year, when seasonal variation is smaller.

```{r}
xtabs(~ station + month, data = recent_data, subset = recent_data$year == 2019)
```
So sampling is not completely equal and we do have two sites in the Haraseeket
with poor data coverage, and a few other  sites missing October data. 
In other analyses, we noted that October often has higher DIN values than other
months, so we drop it from the analysis, since the eneven samples may affect
results.

```{r create_2019_data}
yr_2019_data <- recent_data %>%
  filter(year == 2019)  %>%
  filter(month !='Oct') %>%
  filter(! is.na(din)) %>%
  select(station, station_name, dt, month, doy, din_N)
```

```{r hist_2019}
ggplot(yr_2019_data, aes(din_N)) +
  geom_histogram() +
  scale_x_continuous(trans = 'log')
```

```{r jitter_plot_2019}
ggplot(yr_2019_data, aes(din_N, station_name)) +
  geom_point(aes(color = month)) +
  scale_x_log10()
```

### Calculate Descriptive Statistics
```{r descriptives_2019}
results_2019 <- yr_2019_data %>%
  group_by(station) %>%
  summarize(across(din_N, c(mn = ~ mean(.x, na.rm = TRUE),
                                  sd = ~ sd(.x, na.rm = TRUE), 
                                  n = ~sum(! is.na(.x)),
                                  md = ~ median(.x, na.rm = TRUE),
                                  iqr = ~ IQR(.x, na.rm = TRUE),
                                  p90 = ~ quantile(.x, .9, na.rm = TRUE),
                                  gm = ~ exp(mean(log(.x), na.rm = TRUE))))) %>%
  mutate(station_name = names_df$Alt_Name[match(station,
                                                names_df$Station_ID)]) %>%
  mutate(station = fct_reorder(factor(station), din_N_md),
         station_name = fct_reorder(factor(station_name), din_N_md)) %>%
  relocate(station_name, .after = station)
```

### Linear Model
```{r lm_2019_full}
din_lm_2019_draft <- lm(log(din_N) ~ station *  month , data = yr_2019_data)
anova(din_lm_2019_draft)
```

```{r lm_2019}
din_lm_2019 <- lm(log(din_N) ~ station + month , data = yr_2019_data)
anova(din_lm_2019)
```

```{r}
plot(din_emms_lm) + coord_flip() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) +
  geom_point(data = recent_results, aes(y = as.numeric(station), x = din_N_md),
             color = 'red')
```

#### Marginal Means
```{r marginals_2019}
din_emms_lm_2019 <- emmeans(din_lm_2019, 'station', type = 'response')
plot(din_emms_lm_2019) + coord_flip()+ 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) +
  geom_point(data = results_2019, aes(y = station, x = din_N_md),
             color = 'red')
```

```{r marginals_2019_months}
din_emms_lm_2019_months <- emmeans(din_lm_2019, 'month', type = 'response')
plot(din_emms_lm_2019_months) + coord_flip()+ 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25))
```

### Simplified Linear Model
```{r red_lm_2019}
din_lm_2019_red <- lm(log(din_N) ~ station  , data = yr_2019_data)
anova(din_lm_2019_red)
```

#### Marginal Means
```{r red_2019_marginals}
din_emms_lm_2019_red <- emmeans(din_lm_2019_red, 'station', type = 'response')
plot(din_emms_lm_2019_red) + coord_flip() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25))  +
  geom_point(data = results_2019, aes(y = station, x = din_N_md),
             color = 'red')
```

Qualitatively indistinguishable results....

###  Robust Linear Model
The `rlm()` function from `MASS` implements robust model fitting using M
estimators.  These are estimators that do not use least squares as a criterion
for model fit.  Instead, they use other symmetric functions to quantify the
relative importance of the deviation of each observation from model predictions.
To achieve "robust" qualities, these functions drop off in value at higher
deviations from model predictions, making extreme points count less, or not at 
all, when fitting the model.

Robust linear models, as implemented via `rlm()` from the `MASS`
package do not accept models not of full rank, which is proven a bit of
a problem for these uneven data sets. We can not fit a station + month model.
```{r robust_lm_err_1, error = TRUE}
din_rlm_2019_FAILS <- rlm(log(din_N) ~ station + month, 
                     na.action = na.omit,
                     data = yr_2019_data)
```

```{r robust_lm}
din_rlm_2019 <- rlm(log(din_N) ~ station, 
                     na.action = na.omit,
                     data = yr_2019_data)
```


#### Marginal Means
```{r robust_lm_marginals}
din_emms_lm_2019 <- as_tibble(emmeans(din_lm_2019, 
                                      'station', type = 'response'))
din_emms_rlm_2019 <- as_tibble(emmeans(din_rlm_2019, 
                                      'station', type = 'response'))
```

## GAM model
We can use a GAM model to look at seasonal patterns within this one year, but
this also may be overfitting available data.  We don't fit a cyclic smoother
because our data covers only a small portion of the year.
```{r gam_2019}
din_gam_2019 <- gam(log(din_N) ~ station +  s(doy, bs = 'cs', k = 6), 
               data = yr_2019_data)
anova(din_gam_2019)
```

```{r view_gam_2019}
plot(din_gam_2019)
```
The day of year smoother may be slightly over fit here.

```{r gam_2019_diagnostics}
oldpar <- par(mfrow = c(2,2))
gam.check(din_gam)
par(oldpar)
```
Those diagnostics are not too bad, with the exception of a few large negative 
residuals again.

#### GAM Marginal Means
```{r gam_2019_marginals}
din_emms_gam_2019 <- emmeans(din_gam_2019, 'station', type = 'response')
plot(din_emms_gam_2019) + coord_flip() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25)) +
  geom_point(data = results_2019, aes(y = station, x = din_N_md),
             color = 'red')
din_emms_gam_2019 <- as_tibble(din_emms_gam_2019) 
```
Again, qualitatively similar results, but this model appears to slightly 
overestimate observed means fairly consistently.

### Compare Model Results -- Does Model Selection Matter?
#### Compare Models to Observed Means
##### Log Linear Model
```{r compare_2019_lm}
compare <- results_2019 %>%
  select(station, station_name, contains('tn'), contains('din_N')) %>%
  full_join(din_emms_lm_2019, by = 'station', suffix = c('.data', '.lm'), 
            copy = TRUE)

ggplot(compare, aes(din_N_mn, response)) +
  geom_abline(slope = 1, intercept = 0) + 
  geom_point(size = 3, color = 'blue') +
  geom_linerange(aes(xmin = din_N_mn - 2 * din_N_sd/sqrt(din_N_n), 
                     xmax = din_N_mn + 2 * din_N_sd/sqrt(din_N_n))) +
  geom_linerange(aes(ymin = lower.CL, ymax = upper.CL)) +
  xlab('Observed ') +
  ylab('Linear Model') +
  coord_equal() +
  scale_x_log10()+
  scale_y_log10()
```
The log linear model generally fits means slightly lower than observed.

##### Robust Log Linear Model
```{r compare_2019_rlm}
compare <- results_2019 %>%
  select(station, station_name, contains('tn'), contains('din_N')) %>%
  full_join(din_emms_rlm_2019, by = 'station', suffix = c('.data', '.lm'), 
            copy = TRUE)

ggplot(compare, aes(din_N_mn, response)) +
  geom_abline(slope = 1, intercept = 0) + 
  geom_point(size = 3, color = 'blue') +
  geom_linerange(aes(xmin = din_N_mn - 2 * din_N_sd/sqrt(din_N_n), 
                     xmax = din_N_mn + 2 * din_N_sd/sqrt(din_N_n))) +
  geom_linerange(aes(ymin = asymp.LCL, ymax = asymp.UCL)) +
  xlab('Observed ') +
  ylab('Robust Linear Model') +
  coord_equal() +
  scale_x_log10()+
  scale_y_log10()
```
Results of the robust model are very similar.

##### GAM Model
```{r compare_2019_gam}
compare <- results_2019 %>%
  select(station, station_name, contains('tn'), contains('din_N')) %>%
  full_join(din_emms_gam_2019, by = 'station', suffix = c('.data', '.lm'), 
            copy = TRUE)

ggplot(compare, aes(din_N_mn, response)) +
  geom_abline(slope = 1, intercept = 0) + 
  geom_point(size = 3, color = 'blue') +
  geom_linerange(aes(xmin = din_N_mn - 2 * din_N_sd/sqrt(din_N_n), 
                     xmax = din_N_mn + 2 * din_N_sd/sqrt(din_N_n))) +
  geom_linerange(aes(ymin = lower.CL, ymax = upper.CL)) +
  xlab('Observed ') +
  ylab('GAM Model') +
  coord_equal() +
  scale_x_log10()+
  scale_y_log10()
```

The GAM model provides adjusted estimates that generally lie close to the 
observed means.  Values are slightly higher than the observed means,
reflecting different time of year adjustments from the linear model. These
estimates are effectively adjusted for different sampling histories. Note that
error bars are higher than for the observed means.

#### Compare Log Linear and Robust Log Linear Models
We can show that more clearly by plotting the predictions of the two models 
against one another.
```{r compare_2019_models}
compare <- as_tibble(din_emms_lm_2019) %>%
  full_join(din_emms_rlm_2019, by = 'station', suffix = c('.lm', '.rlm'))
ggplot(compare, aes(response.lm, response.rlm)) +
  geom_abline(slope = 1, intercept = 0) + 
  geom_point(size = 3, color = 'blue') +
  geom_linerange(aes(xmin = lower.CL, xmax = upper.CL)) +
  geom_linerange(aes(ymin = asymp.LCL, ymax = asymp.UCL)) +
  xlab('Linear Model') +
  ylab('Robust Linear Model') +
  coord_equal() +
  scale_x_log10()+
  scale_y_log10()
```
So, results are qualitatively similar. There is no strong reason to prefer the 
robust estimates to the linear model estimates where qualitative results are 
similar and model diagnostics are fairly good.

# Restricted Data -- Core Months
Another way to focus our analysis is to focus only on the "warmer" months, which 
sampled more consistently.  While most sites were sampled in October, a few
were not, so we focus on May through September.
```{r create_core_months_data}
core_months_data <- recent_data %>%
  filter(month %in% month.abb[5:9])  %>%
  filter(! is.na(din)) %>%
  select(station, station_name, dt, year, yearf, month, doy, din_N)
```

```{r}
xtabs(~ station + month, data = core_months_data)
```

We see a few little sampled sites, especially in the Haraseeket, but otherwise,
representation is fairly uniform.

```{r hist_core_months}
ggplot(core_months_data, aes(din_N)) +
  geom_histogram() +
  scale_x_continuous(trans = 'log10')
```

```{r jitter_plot_core_months}
ggplot(core_months_data, aes(din_N, station_name)) +
  geom_point(aes(color = yearf), alpha = 0.25) +
  scale_x_log10()
```

We have data only from 2019 from many sites, so this analysis has many
"empty cells" too, if we model year to year variation.

### Calculate Descriptive Statistics
```{r descriptives_core_months}
results_core_months <- core_months_data %>%
  group_by(station) %>%
  summarize(across(din_N, c(mn = ~ mean(.x, na.rm = TRUE),
                                  sd = ~ sd(.x, na.rm = TRUE), 
                                  n = ~sum(! is.na(.x)),
                                  md = ~ median(.x, na.rm = TRUE),
                                  iqr = ~ IQR(.x, na.rm = TRUE),
                                  p90 = ~ quantile(.x, .9, na.rm = TRUE),
                                  gm = ~ exp(mean(log(.x), na.rm = TRUE))))) %>%
  mutate(station_name = names_df$Alt_Name[match(station,
                                                names_df$Station_ID)]) %>%
  mutate(station = fct_reorder(factor(station), din_N_md),
         station_name = fct_reorder(factor(station_name), din_N_md)) %>%
  relocate(station_name, .after = station)
```

### Linear Model
```{r lm_core_months_full}
din_lm_core_months_draft <- lm(log(din_N) ~ station *  month + year, 
                               data = core_months_data)
anova(din_lm_core_months_draft)
```

```{r lm_core_months}
din_lm_core_months <- lm(log(din_N) ~ station + month + year , data = core_months_data)
anova(din_lm_core_months)
```

```{r core_lm_diagnostics}
oldpar <- par(mfrow = c(2,2))
plot(din_lm_core_months)
par(oldpar)
```
The log transform is slightly too strong for these data, but most 
alternatives are no better.  This model is adequate for our purposes.

#### Marginal Means
```{r marginals_core}
din_emms_lm_core_months <- emmeans(din_lm_core_months, 'station', type = 'response')
plot(din_emms_lm_core_months) + coord_flip()+ 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25))  +
  geom_point(data = results_core_months, aes(y = as.numeric(station), x = din_N_md),
             color = 'red')
```

```{r marginals_core_months}
din_emms_lm_core_months_months <- emmeans(din_lm_core_months, 'month', 
                                          type = 'response')
plot(din_emms_lm_core_months_months) + coord_flip()+ 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25))
```

### Simplified Linear Model
```{r red_lm_core_months}
din_lm_core_months_red <- lm(log(din_N) ~ station, data = core_months_data)
anova(din_lm_core_months_red)
```

#### Marginal Means
```{r red_core_marginals}
din_emms_lm_core_months_red <- emmeans(din_lm_core_months_red, 'station', type = 'response')
plot(din_emms_lm_core_months_red) + coord_flip() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25))  +
  geom_point(data = results_core_months, aes(y = as.numeric(station), x = din_N_md),
             color = 'red') +
  geom_point(data = results_core_months, aes(y = as.numeric(station), x = din_N_gm),
             color = 'blue')
```

Many of the differences from observed means are retained by even the simplest
linear model.  Results are not appreciably closer to the geometric means.

With no covariates, why is this model not just fitting each station perfectly?
Since there is a parameter for each station, means (or here, geometric means) 
should match...

###  Robust Linear Model
Robust linear models, as implemented via `rlm()` from the `MASS`
package do not accept models not of full rank, which is proven a bit of
a problem for these uneven data sets. We can not fit a station + month model.
```{r robust_lm_err_2, error = TRUE}
din_rlm_core_months_FAILS <- rlm(log(din_N) ~ station + month, 
                     na.action = na.omit,
                     data = core_months_data)
```

```{r robust_lm_core}
din_rlm_core_months <- rlm(log(din_N) ~ station, 
                     na.action = na.omit,
                     data = core_months_data)
```


#### Marginal Means
```{r robust_lm_core_marginals}
din_emms_lm_core_months <- as_tibble(emmeans(din_lm_core_months, 
                                      'station', type = 'response'))
din_emms_rlm_core_months <- as_tibble(emmeans(din_rlm_core_months, 
                                      'station', type = 'response'))
```

## GAM model
We can use a GAM model to look at seasonal patterns within this one year, but
this also may be overfitting available data.  We don't fit a cyclic smoother
because our data covers only a small portion of the year.
```{r gam_core_months}
din_gam_core_months <- gam(log(din_N) ~ station +  s(doy, bs = 'cs', k = 6), 
               data = core_months_data)
anova(din_gam_core_months)
```

```{r view_gam_core_months}
plot(din_gam_core_months)
```

```{r gam_core_months_diagnostics}
oldpar <- par(mfrow = c(2,2))
gam.check(din_gam)
par(oldpar)
```
Those diagnostics are not too bad, although the log transform is a bit strong.

#### GAM Marginal Means
```{r gam_core_months_marginals}
din_emms_gam_core_months <- emmeans(din_gam_core_months, 'station', type = 'response')
plot(din_emms_gam_core_months) + coord_flip() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25))
din_emms_gam_core_months <- as_tibble(din_emms_gam_core_months)
```


### Compare Model Results -- Does Model Selection Matter?
#### Compare Models to Observed Means
##### Log Linear Model
```{r compare_core_months_lm}
compare <- results_core_months %>%
  select(station, station_name, contains('tn'), contains('din_N')) %>%
  full_join(din_emms_lm_core_months, by = 'station', suffix = c('.data', '.lm'), 
            copy = TRUE)

ggplot(compare, aes(din_N_mn, response)) +
  geom_abline(slope = 1, intercept = 0) + 
  geom_point(size = 3, color = 'blue') +
  geom_linerange(aes(xmin = din_N_mn - 2 * din_N_sd/sqrt(din_N_n), 
                     xmax = din_N_mn + 2 * din_N_sd/sqrt(din_N_n))) +
  geom_linerange(aes(ymin = lower.CL, ymax = upper.CL)) +
  xlab('Observed ') +
  ylab('Robust Linear Model') +
  coord_equal() +
  scale_x_log10()+
  scale_y_log10()
```
The log linear model generally fits means slightly lower than observed. This 
is what is expected as we are effectively fitting geometric means instead of
arithmetic means.

##### Robust Log Linear Model
```{r compare_core_months_rlm}
compare <- results_core_months %>%
  select(station, station_name, contains('tn'), contains('din_N')) %>%
  full_join(din_emms_rlm_core_months, by = 'station', suffix = c('.data', '.lm'), 
            copy = TRUE)

ggplot(compare, aes(din_N_mn, response)) +
  geom_abline(slope = 1, intercept = 0) + 
  geom_point(size = 3, color = 'blue') +
  geom_linerange(aes(xmin = din_N_mn - 2 * din_N_sd/sqrt(din_N_n), 
                     xmax = din_N_mn + 2 * din_N_sd/sqrt(din_N_n))) +
  geom_linerange(aes(ymin = asymp.LCL, ymax = asymp.UCL)) +
  xlab('Observed ') +
  ylab('Robust Linear Model') +
  coord_equal() +
  scale_x_log10()+
  scale_y_log10()
```
Results of the robust model are very similar.

##### GAM Model
```{r compare_core_months_gam}
compare <- results_core_months %>%
  select(station, station_name, contains('tn'), contains('din_N')) %>%
  full_join(din_emms_gam_core_months, by = 'station', suffix = c('.data', '.lm'), 
            copy = TRUE)

ggplot(compare, aes(din_N_mn, response)) +
  geom_abline(slope = 1, intercept = 0) + 
  geom_point(size = 3, color = 'blue') +
  geom_linerange(aes(xmin = din_N_mn - 2 * din_N_sd/sqrt(din_N_n), 
                     xmax = din_N_mn + 2 * din_N_sd/sqrt(din_N_n))) +
  geom_linerange(aes(ymin = lower.CL, ymax = upper.CL)) +
  xlab('Observed ') +
  ylab('GAM Model') +
  coord_equal() +
  scale_x_log10()+
  scale_y_log10()
```

The GAM model provides adjusted estimates that generally lie close to the 
observed means.  These estimates are effectively adjusted for different sampling 
histories.  Note that error bars are higher than for the straight means.

#### Compare Log Linear and Robust Log Linear Models
We can show that more clearly by plotting the predictions of the two models 
against one another.
```{r compare_core_months_models}
compare <- as_tibble(din_emms_lm_core_months) %>%
  full_join(din_emms_rlm_core_months, by = 'station', suffix = c('.lm', '.rlm'))
ggplot(compare, aes(response.lm, response.rlm)) +
  geom_abline(slope = 1, intercept = 0) + 
  geom_point(size = 3, color = 'blue') +
  geom_linerange(aes(xmin = lower.CL, xmax = upper.CL)) +
  geom_linerange(aes(ymin = asymp.LCL, ymax = asymp.UCL)) +
  xlab('Linear Model') +
  ylab('Robust Linear Model') +
  coord_equal() +
  scale_x_log10()+
  scale_y_log10()
```


# DIN  Recent Condition Conclusions
Restricting attention to 2019 makes sense. An alternative restriction to
selected months left many stations with only data from 2019 anyway.

There appears to be little advantage to robust models.  Even so, there are 
several approaches possible:  
1.  Just use observed means / medians.  
2.  Use fitted means from the simplest linear models.  This effectively fits
    geometric means, not arithmetic means, and pools error estimates.   
3.  Use Marginal Means from the GAM model -- these are effectively adjusted for
    different  sampling histories, especially time of year.

# Trend Analysis
Few stations have data from more than a few years.  DIN data has been collected 
over the past couple of years, at several stations in the mid 200s, and at a
handful of stations pretty much every year since 2001.  Generally the rule we 
have used to examine trends is to focus on sites with relatively complete 
records, say at least two of the last five years and at least
ten years total.  

## Identify Trend Stations
```{r which_stations}
trend_sites <- strict_data %>%
  group_by(station, year) %>%
  summarize(was_sampled =  ! all(is.na(din_N)),
            .groups = 'drop_last') %>%
  summarize(last_5 = sum(was_sampled & year > 2014),
            total = sum(was_sampled),
            .groups = 'drop') %>%
  filter(total >= 10, last_5 >= 2) %>%
  pull(station)
trend_sites
```

## Generate Trend Data
```{r make_trend_data}
trend_data <- strict_data %>%
  filter(station %in% trend_sites) %>%
  filter(! is.na(din_N)) %>%
   mutate(station_name = names_df$Alt_Name[match(station,
                                                names_df$Station_ID)]) %>%
   mutate(station = factor(station),
          station_name = factor(station_name)) %>%
   mutate(station = fct_reorder(station, din_N, na.rm = TRUE),
         station_name = fct_reorder(station_name, din_N, na.rm = TRUE)) %>%
   relocate(station_name, .after = station) %>%
   select(-tn_depth, -tn, -organic_N)
```

## Data Prevalence
```{r trend_data_months}
xtabs(~ month + station, data = trend_data )%>%
  as_tibble() %>%
  mutate(month = factor(month, levels = month.abb)) %>%
  filter(n>0) %>%

  ggplot(aes(station, month, fill = sqrt(n))) +
  geom_tile() +
  theme_cbep(base_size = 12) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .25))
```
So we have few winter samples from four of these sites, suggesting we may want 
to look at a more limited subset of the data to avoid introducing bias into
our models.  The core data is from April to October, with fairly consistent
level of effort across sites.  We could focus on sites 
with more complete records, or focus on months with more sites.

```{r trend_data_years}
xtabs(~ year + station, data = trend_data) %>%
  as_tibble() %>% 
  filter(n>0) %>%

  ggplot(aes(station, year, fill = sqrt(n))) +
  geom_tile() +
  theme_cbep(base_size = 12) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .25))
```
The same station have more complete records by year, suggesting we may also want 
to the define five core trend stations where data is more complete.

```{r trend_data_times}
xtabs(~ year + month, data = trend_data) %>%
  as_tibble() %>% 
  mutate(month = factor(month, levels = month.abb))  %>%
  filter(n>0) %>%

  ggplot(aes(month, year, fill = sqrt(n))) +
  geom_tile() +
  theme_cbep(base_size = 12) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .25))
```

Most winter samples are older, but we have a few winter samples right up until
the last few years. The lack of recent winter data means we need to be cautious 
about interpreting long-term trends, because we know from prior analyses that 
din_N values tend to be lower in the summer months.

We are mostly interested in month (or more generally, time of year) as a
covariate. Although seasonal variation is complex, probably 
differing year to year and site to site, this is a place where we may be able to
fit a hierarchical model to assist with bridging some data gaps.

# Core Site Trend Data
```{r find_core_sites}
core_sites <- strict_data %>%
  group_by(station, year) %>%
  summarize(was_sampled =  ! all(is.na(din_N)),
            .groups = 'drop_last') %>%
  summarize( total = sum(was_sampled),
            .groups = 'drop') %>%
  filter(total >= 15) %>%
  pull(station)
core_sites
```

```{r make_core_sites_data}
core_sites_data <- trend_data %>%
  filter(station %in% core_sites)
```

## Jitter Plot By Year
```{r core_sites_jitter_plot_by_yr, fig.height = 7, fig.width = 3}
ggplot(core_sites_data, aes(year, din_N,)) +
  geom_jitter(aes( color = station_name), alpha = 0.5) + 
  stat_summary(fun = mean, geom = 'line', lwd = 1) +
  #scale_y_continuous(trans = 'log') +
  facet_wrap(~station_name, nrow = 5) +
  scale_color_manual(values = cbep_colors()) +
  theme_cbep(base_size = 12) +
  theme(legend.position = 'None')
```

We note a small number of nominal zero values in 2003 and 2004.  We note that
neither log-transforms nor  transformed data conform well to assumptions of
normality, but the log plus one transform works well.

We note no obvious linear pattern to the annual means, but there is a possible 
drop in annual averages in recent years. Unfortunately, that drop corresponds
to years when winter samples are no longer being collected.

## Real Dates and Times
First, we create a summary dataframe
```{r create_summary}
core_site_summary  <-   core_sites_data %>%
  select(station_name, year, din_N) %>%
  group_by(station_name, year) %>%
  summarize(ann_mn_din_N = mean(din_N, na.rm = TRUE),
            .groups = 'drop_last') %>%
  filter(! is.na(ann_mn_din_N)) %>%
  mutate(dt = as.Date (paste0('06-15-', year), format = '%m-%d-%Y'))
```

```{r core_sites_plot_by_date, fig.height = 7, fig.width = 3}
ggplot(core_sites_data) +
  geom_point(aes(dt, din_N, color = station_name), alpha = 0.5) + 
  geom_line(data = core_site_summary, 
            mapping = aes(x = dt, y = ann_mn_din_N), 
            lwd = 1,
            color = cbep_colors()[3]) +
  #scale_y_continuous(trans = 'log') +
  facet_wrap(~station_name, nrow = 5) +
  scale_color_manual(values = cbep_colors()) +
  theme_cbep(base_size = 12) +
  theme(legend.position = 'None') +
  xlab('') +
  ylab('DIN (mg/l as N)')
```

# Core Months Trend Data
```{r make_core_months_data}
core_months_data <- trend_data %>%
  filter(month %in% month.abb[5:10])
```


## Jitter Plot By Year
```{r core_months_jitter_plot_by_yr, fig.height = 7, fig.width = 5}
ggplot(core_months_data, aes(year, din_N,)) +
  geom_jitter(aes(color = station_name)) + 
  stat_summary(fun = mean, geom = 'line', lwd = 1) +
  #scale_y_continuous(trans = 'log1p') +
  facet_wrap(~station_name, nrow = 4) +
  scale_color_viridis_d(option = 'viridis') +
  theme_cbep(base_size = 12) +
  theme(legend.position = 'None',
        panel.spacing.x = unit(2.5, "lines"))
```

##Actual Dates
```{r make_core_months_summary}
core_months_summary  <-   core_months_data %>%
  select(station_name, year, din_N) %>%
  group_by(station_name, year) %>%
  summarize(ann_mn_din_N = mean(din_N, na.rm = TRUE),
            .groups = 'drop_last') %>%
  filter(! is.na(ann_mn_din_N)) %>%
  mutate(dt = as.Date (paste0('06-15-', year), format = '%m-%d-%Y'))
```

```{r core_months_jitter_plot_by_date, fig.height = 7, fig.width = 5}
ggplot(core_months_data) +
  geom_point(aes(dt, din_N, color = station_name), alpha = 0.5) + 
  geom_line(data = core_months_summary, 
            mapping = aes(x = dt, y = ann_mn_din_N), 
            lwd = 1,
            color = cbep_colors()[3]) +
  scale_y_continuous(trans = 'log1p') +
  facet_wrap(~station_name, nrow = 4) +
  scale_color_viridis_d(option = 'viridis') +
  theme_cbep(base_size = 12) +
  theme(legend.position = 'None',
        panel.spacing.x = unit(2.5, "lines")) +
  xlab('') +
  ylab('DIN (mg/l as N)')
```

# Models
## Initial Linear Model
Note that in this setting, there is no reason to believe all stations show the 
same trend, so a model that does not fit an interaction term (station x year)
is of limited value.  The exception may be use of hierarchical model that treats
sites as random factors.
```{r trend_lm_1}
trnd_lm_1 <- lm(log1p(din_N) ~ (year + station + month)^2 , 
                data = core_months_data)
anova(trnd_lm_1)
```

```{r trend_lm_step}
trnd_lm_2 <- step(trnd_lm_1)
```

```{r anova_trend_lm_2}
anova(trnd_lm_2)
```

```{r summary_trend_lm_2}
summary(trnd_lm_2)
```

So the obvious linear model analysis suggests there is a weak positive linear
trend, and there are no differences in trend among stations.  But that misses 
what may be a recent decline.

```{r trend_lm_2_diagnostics}
oldpar <- par(mfrow=c(2,2))
plot(trnd_lm_2)
par(oldpar)
```
Other than the skewness of the residuals, model diagnostics
are pretty good, suggesting these conclusions will be robust to most other
reasonable model specifications.

## Check for Non-linear Patterns
We start by fitting a polynomial 
```{r trend_polynomial}
trnd_lm_3 <- lm(log1p(din_N) ~ poly(year,2) + poly(year,2):station + 
                                month + month:year, data = core_months_data)
anova(trnd_lm_3)
```

So, there is evidence for non-linear changes over time. The problem is, we 
expect year to year changes due to things like weather, so it is hard to 
evaluate whether any of this matters.  To get a handle on that, we turn to a GAM 
model, .

```{r trend_gam_1}
trnd_gam_1 <- gam(din_N ~ station + month +
                    s(year, by = station, k = 5),
                  data = core_months_data)
anova(trnd_gam_1)
```
```{r trend_gam_2}
trnd_gam_2 <- gam(din_N ~ station + month +
                    s(year, k = 5),
                  data = core_months_data)
anova(trnd_gam_1, trnd_gam_2)
AIC(trnd_gam_1, trnd_gam_2)
```

SO there's a large increase in deviance going to the simpler model (on the order 
of 5%) and AIC goes up substantially.  We need the more complex model.

```{r view_gam_smoothers}
plot(trnd_gam_1, se = FALSE, trans = log1p)
```

These effects are relatively large, compared to the means, but the patterns are
different from site to site.  In other words, there is little here to
interpret that would add insight to readers.  The conclusion is, there are
year to year differences, that differ site to site, and thus no real trends.

It is possible a formal time series analysis might uncover autocorrelation
structure, but that is well beyond our current needs.


